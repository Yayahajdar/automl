### **Rapport de Monitoring et Gestion d'Incidents**

**Projet :** AutoML-Gr4-1 - Analyseur de CSV Automatisé

**Auteur :** Ahmed Yahya Haj Darwish
**Date :** 31/08/2025
**Version :** 1.0

---

### **SOMMAIRE**

1.  **Préambule**
2.  **Introduction**
3.  **Monitoring de l’Application**
    *   3.1. Définition des Métriques Clés (KPIs)
    *   3.2. Définition des Seuils et Alertes
    *   3.3. Solution de Monitoring : Architecture et Outils
4.  **Résolution d’Incidents Techniques : Étude de Cas**
    *   4.1. Définition de l’Incident : Erreur 500 Simulée
    *   4.2. Identification de la Cause : Détection et Alerte
    *   4.3. Solution : Correction et Rétablissement du Service
    *   4.4. Analyse Post-Mortem
5.  **Conclusion**
6.  **Annexes**

---

### **1. Préambule**

Ce document a pour objectif de présenter la stratégie de monitoring et de gestion des incidents mise en place pour le projet **AutoML-Gr4-1**. Il détaille les métriques surveillées, l'architecture de la solution de monitoring, et illustre le processus de résolution d'incident à travers une étude de cas pratique, démontrant ainsi la robustesse et la maintenabilité de l'application.

### **2. Introduction**

Le projet AutoML-Gr4-1 est une plateforme web permettant aux utilisateurs de téléverser des fichiers CSV, d'entraîner automatiquement des modèles de Machine Learning et d'obtenir des prédictions. Étant donné la nature critique du traitement des données et de la disponibilité du service, une stratégie de surveillance proactive est essentielle pour garantir une haute disponibilité, des performances optimales et une détection rapide des anomalies. Ce rapport formalise cette stratégie.

### **3. Monitoring de l’Application**

#### **3.1. Définition des Métriques Clés (KPIs)**

Pour assurer la santé de l'application, nous surveillons les métriques suivantes :

*   **Disponibilité de l'application :** Le service web est-il accessible ? (Métrique `up`).
*   **Taux d'erreurs HTTP :** Pourcentage de requêtes se terminant par une erreur (codes 5xx). Une augmentation soudaine indique un problème backend.
*   **Latence des requêtes :** Temps de réponse moyen et au 95ème percentile. Une latence élevée dégrade l'expérience utilisateur.
*   **Utilisation des ressources système (via Node Exporter) :**
    *   **CPU :** Pourcentage d'utilisation. Un pic prolongé peut indiquer une boucle infinie ou une charge excessive.
    *   **Mémoire (RAM) :** Utilisation de la mémoire. Une fuite de mémoire peut entraîner une panne du service.
*   **Métriques applicatives personnalisées :**
    *   `app_errors_total` : Compteur spécifique pour les erreurs critiques générées intentionnellement ou capturées dans le code, permettant de distinguer les types d'erreurs.

#### **3.2. Définition des Seuils et Alertes**

Les alertes sont configurées dans Grafana pour notifier l'équipe via Discord lorsque les seuils suivants sont dépassés :

| Métrique | Seuil d'Alerte | Priorité | Description de l'Alerte |
| :--- | :--- | :--- | :--- |
| **Taux d'erreurs 5xx** | `rate(http_requests_total{status=~"5.."}[5m]) > 0.1` | Haute | Le taux d'erreurs serveur a dépassé 10% sur les 5 dernières minutes. |
| **Erreur simulée** | `rate(app_errors_total{type="simulated"}[1m]) > 0` | Critique | Une erreur critique simulée a été détectée. |
| **Utilisation CPU** | `avg_over_time(node_cpu_seconds_total[5m]) > 80%` | Moyenne | L'utilisation du CPU est supérieure à 80% depuis 5 minutes. |
| **Service indisponible** | `up{job="django"} == 0` | Critique | Le service Django ne répond plus au scraping de Prometheus. |

#### **3.3. Solution de Monitoring : Architecture et Outils**

Notre solution est entièrement conteneurisée avec Docker Compose et s'articule autour des outils open-source suivants :

*   **Prometheus :** Collecte et stocke les métriques en "scrapant" périodiquement l'endpoint `/metrics` exposé par l'application Django (via `django-prometheus`) et les métriques système de `Node Exporter`.
*   **Grafana :** Se connecte à Prometheus comme source de données pour visualiser les métriques via des tableaux de bord et pour gérer le système d'alertes.
*   **Discord :** Agit comme point de contact pour les notifications d'alerte envoyées par Grafana, permettant une communication centralisée.
*   **Node Exporter :** Expose les métriques du système d'exploitation de l'hôte (CPU, RAM, disque).

**Schéma d'Architecture :**
*(Ici, vous inséreriez le schéma Mermaid que nous avons généré précédemment)*

### **4. Résolution d’Incidents Techniques : Étude de Cas**

Cette section décrit la gestion d'un incident simulé de bout en bout.

#### **4.1. Définition de l’Incident : Erreur 500 Simulée**

Pour tester notre chaîne de monitoring, un endpoint `/simulate-error/` a été créé. Lorsqu'il est appelé, il génère intentionnellement une erreur HTTP 500 et incrémente le compteur Prometheus `app_errors_total{type="simulated"}`.

**Reproduction de l'incident :**
L'incident est déclenché via une simple commande `curl` :
```bash
curl http://localhost:8000/simulate-error/
```

#### **4.2. Identification de la Cause : Détection et Alerte**

1.  **Détection :** Immédiatement après l'appel `curl`, Prometheus enregistre une augmentation du compteur `app_errors_total`.
2.  **Alerte :** La règle d'alerte dans Grafana, basée sur la requête PromQL `rate(app_errors_total{type="simulated"}[1m]) > 0`, se déclenche.
3.  **Notification :** Grafana envoie une notification détaillée à notre canal Discord dédié, informant l'équipe de l'incident critique en temps réel. (Voir Annexe B pour la capture d'écran de l'alerte).

#### **4.3. Solution : Correction et Rétablissement du Service**

*   **Diagnostic :** L'alerte pointe directement vers une erreur de type "simulated". Une analyse des logs applicatifs ou du tableau de bord Grafana (voir Annexe A) confirme que l'erreur provient de l'endpoint `/simulate-error/`.
*   **Cause Racine :** Le code de la vue `simulate_error` dans `csv_processor/views.py` est identifié comme la source du problème (voir Annexe C).
*   **Résolution :** Dans un cas réel, la résolution impliquerait de corriger le code défectueux. Pour cet incident simulé, la "résolution" consiste à accuser réception de l'alerte et à documenter l'événement. Si l'erreur avait paralysé le service, une mesure immédiate aurait pu être de redémarrer le conteneur `web` :
    ```bash
    docker-compose restart web
    ```
*   **Vérification :** Une fois l'alerte résolue, Grafana envoie une notification de rétablissement `[RESOLVED]` sur Discord.

#### **4.4. Analyse Post-Mortem**

*   **Impact :** L'incident a provoqué une erreur 500 pour les utilisateurs accédant à un endpoint spécifique, sans affecter le reste de l'application.
*   **Cause Racine :** La cause était une erreur délibérément introduite dans le code à des fins de test.
*   **Actions Correctives :** La chaîne de monitoring (détection, alerte, notification) a fonctionné comme prévu. Aucune action corrective sur l'outillage n'est nécessaire.
*   **Mesures Préventives :** Cet exercice valide notre capacité à détecter et à réagir rapidement aux erreurs applicatives critiques.

### **5. Conclusion**

La mise en place de cette pile de monitoring robuste avec Prometheus et Grafana nous offre une visibilité complète sur la santé de notre application. L'étude de cas démontre que nous sommes capables de détecter, diagnostiquer et réagir à des incidents techniques de manière efficace, minimisant ainsi l'impact sur l'expérience utilisateur et garantissant la fiabilité du service AutoML-Gr4-1.

### **6. Annexes**

**Annexe A : Capture d'écran du Tableau de Bord Grafana**
*(Insérer ici une capture d'écran de Grafana montrant le pic de la métrique `app_errors_total`)*

**Annexe B : Capture d'écran de l'Alerte sur Discord**
*(Insérer ici une capture d'écran du message d'alerte reçu sur Discord)*

**Annexe C : Extraits de Code Pertinents**

*   **`csv_processor/metrics.py` : Définition du compteur**
    ```python
    from prometheus_client import Counter
    APP_ERRORS_TOTAL = Counter(
        'app_errors_total',
        'Total number of application errors.',
        ['type']
    )
    ```

*   **`csv_processor/views.py` : Vue simulant l'erreur**
    ```python
    from django.http import JsonResponse
    from .metrics import APP_ERRORS_TOTAL

    def simulate_error(request):
        APP_ERRORS_TOTAL.labels(type='simulated').inc()
        return JsonResponse({'status': 'error', 'message': 'This is a simulated 500 error.'}, status=500)
    ```

*   **`csv_processor/urls.py` : Déclaration de l'URL**
    ```python
    from django.urls import path
    from . import views

    urlpatterns = [
        # ... autres urls
        path('simulate-error/', views.simulate_error, name='simulate-error'),
    ]
    ```

**Annexe D : Requête PromQL pour l'Alerte**
```promql
rate(app_errors_total{type="simulated"}[1m]) > 0
```

---
