### **Rapport de Monitoring et Gestion d'Incidents (Livrable E5)**

---

### **SOMMAIRE**

1.  **Pr√©ambule**
2.  **Introduction**
    *   Objectif et Fonctionnement de l'Application
    *   Sch√©ma d'Architecture G√©n√©ral
3.  **Monitoring de l‚ÄôApplication**
    *   D√©finition des M√©triques Cl√©s
    *   D√©finition des Seuils et Alertes
    *   Solution de Monitoring (Prometheus & Grafana)
4.  **R√©solution d‚ÄôIncidents Techniques : √âtude de Cas**
    *   D√©finition de l'Incident : Erreur 500 Simul√©e
    *   Identification de la Cause
    *   Solution et R√©solution
5.  **Conclusion**
6.  **Annexes**
    *   Annexe A: Configuration `prometheus.yml`
    *   Annexe B: Requ√™tes PromQL
    *   Annexe C: Configuration de l'Alerte Grafana

---

### **1. Pr√©ambule**

Ce document constitue le livrable E5, r√©pondant aux comp√©tences C20 ("Mettre en place un syst√®me de surveillance (monitoring) des performances et des erreurs de l'application") et C21 ("G√©rer les incidents techniques et appliquer les correctifs n√©cessaires"). Il d√©taille la strat√©gie de monitoring, la gestion des incidents et pr√©sente une √©tude de cas pratique sur la plateforme AutoML.

### **2. Introduction**

#### **Objectif et Fonctionnement de l'Application**

Notre application, **AutoML Analyzer**, est une plateforme web con√ßue pour simplifier le processus de Machine Learning. Elle permet aux utilisateurs de t√©l√©verser des fichiers CSV, de s√©lectionner des mod√®les d'apprentissage automatique (classification ou r√©gression), de les entra√Æner sur leurs donn√©es et d'obtenir des pr√©dictions. L'objectif est de d√©mocratiser l'acc√®s √† l'AutoML en fournissant une interface intuitive et un pipeline de traitement automatis√©.

L'application est construite avec **Django** pour le backend, et l'ensemble de l'√©cosyst√®me (base de donn√©es, serveur web, outils de monitoring) est conteneuris√© avec **Docker**.

#### **Sch√©ma d'Architecture G√©n√©ral**

L'architecture de notre projet est con√ßue pour √™tre modulaire et scalable. Elle int√®gre des services pour l'application web, la base de donn√©es, le suivi des exp√©riences ML, et une pile de monitoring compl√®te.

`[DIAGRAM: Sch√©ma d'architecture g√©n√©ral du projet, bas√© sur le code Mermaid corrig√© que je vous ai fourni pr√©c√©demment. Il doit montrer les interactions entre l'utilisateur, Docker, Django, PostgreSQL, MLflow, Prometheus, Grafana et les volumes locaux.]`

### **3. Monitoring de l‚ÄôApplication**

#### **D√©finition des M√©triques Cl√©s**

Pour assurer la fiabilit√© et la performance de l'application, nous avons choisi de surveiller les m√©triques suivantes, jug√©es les plus pertinentes :

1.  **`up` (Disponibilit√© des services)**: M√©trique binaire (0 ou 1) fournie par Prometheus pour chaque service (cible). Elle est fondamentale pour savoir si nos services (application web, node-exporter) sont en ligne et accessibles.
2.  **`rate(django_http_requests_total_by_method_path_total[5m])` (Taux de requ√™tes HTTP)**: Fournie par `django-prometheus`, cette m√©trique nous donne le nombre de requ√™tes par seconde. Elle est cruciale pour suivre la charge de l'application et d√©tecter les pics de trafic anormaux.
3.  **`rate(django_http_exceptions_total_by_type_total[5m])` (Taux d'erreurs serveur)**: √âgalement de `django-prometheus`, elle compte les exceptions non g√©r√©es (typiquement les erreurs 5xx). C'est un indicateur direct de la sant√© de l'application.
4.  **`app_errors_total` (Erreurs applicatives sp√©cifiques)**: C'est un compteur personnalis√© que nous avons cr√©√© pour tracer des erreurs m√©tier sp√©cifiques, comme notre erreur 500 simul√©e. Il nous permet de cr√©er des alertes tr√®s cibl√©es.
5.  **`node_cpu_seconds_total` (Utilisation du CPU)**: Fournie par `Node Exporter`, cette m√©trique est essentielle pour surveiller la consommation des ressources syst√®me et pr√©venir la saturation du serveur.

#### **D√©finition des Seuils et Alertes**

Les seuils ont √©t√© d√©finis pour d√©tecter proactivement les probl√®mes avant qu'ils n'impactent gravement les utilisateurs.

*   **Alerte 1: Service Indisponible (Priorit√©: Critique)**
    *   **Seuil**: `up == 0` pendant plus d'une minute.
    *   **Pertinence**: Une indisponibilit√©, m√™me courte, est l'incident le plus critique. Une minute permet de filtrer les red√©marrages planifi√©s ou les faux positifs tr√®s brefs.
*   **Alerte 2: Taux d'Erreurs 5xx √âlev√© (Priorit√©: Haute)**
    *   **Seuil**: `rate(app_errors_total{type="simulated"}[5m]) > 0`
    *   **Pertinence**: Cette alerte est sp√©cifique √† notre incident simul√©. Le seuil `> 0` signifie que nous voulons √™tre notifi√©s d√®s la **premi√®re occurrence** d'une erreur de ce type, car elle est anormale et indique un bug.

#### **Solution du Monitoring**

Nous avons opt√© pour la pile **Prometheus + Grafana**, une solution open-source standard de l'industrie, pour les raisons suivantes :

*   **Int√©gration native**: `django-prometheus` offre une int√©gration simple et expose une grande vari√©t√© de m√©triques pertinentes pour Django.
*   **Puissance de PromQL**: Le langage de requ√™te de Prometheus (PromQL) est extr√™mement flexible pour l'analyse et l'agr√©gation de m√©triques.
*   **Visualisation avanc√©e**: Grafana permet de cr√©er des tableaux de bord (dashboards) riches et des alertes complexes.
*   **√âcosyst√®me Docker**: Ces outils s'int√®grent parfaitement dans notre environnement Docker.

**Fonctionnement :**
1.  **Collecte**: Prometheus est configur√© pour "scraper" (collecter) p√©riodiquement les m√©triques expos√©es par l'application Django sur l'endpoint `/metrics` et par Node Exporter.
2.  **Journalisation/Stockage**: Prometheus stocke ces m√©triques dans sa base de donn√©es de s√©ries temporelles.
3.  **Dashboard**: Grafana se connecte √† Prometheus comme source de donn√©es et affiche les m√©triques sur des tableaux de bord.

`[IMAGE: Capture d'√©cran du dashboard Grafana principal, montrant les graphiques pour le taux de requ√™tes, l'utilisation du CPU et le panel pour les erreurs.]`

### **4. R√©solution d‚ÄôIncidents Techniques : √âtude de Cas**

Nous allons simuler un incident pour d√©montrer l'efficacit√© de notre cha√Æne de monitoring et d'alerte.

#### **D√©finition de l‚ÄôIncident**

*   **Type d'incident**: Une nouvelle fonctionnalit√© d√©ploy√©e introduit un bug qui provoque une erreur HTTP 500 (Internal Server Error) sur un endpoint sp√©cifique.
*   **D√©tection**: L'incident est d√©tect√© par notre syst√®me de monitoring. Une alerte est automatiquement d√©clench√©e dans Grafana lorsque le taux d'erreurs (`app_errors_total`) d√©passe 0, et une notification est envoy√©e sur un canal Discord d√©di√©.

`[IMAGE: Capture d'√©cran de la notification d'alerte re√ßue sur Discord, montrant le message "FIRING: High Error Rate Detected".]`

#### **Identification de la/les Cause(s)**

1.  **Investigations**: En recevant l'alerte, la premi√®re √©tape est de consulter le dashboard Grafana. Le panel "Error Rate" montre un pic soudain pour la m√©trique `app_errors_total{type="simulated"}`.

    `[IMAGE: Capture d'√©cran du graphique Grafana montrant le pic de la m√©trique app_errors_total au moment de l'incident.]`

2.  **Reproduction de l'incident**: Le message d'alerte et les labels de la m√©trique nous indiquent que l'erreur est de type "simulated". Pour reproduire l'incident en d√©veloppement, nous utilisons la commande `curl` qui a √©t√© cr√©√©e √† cet effet :
    ```bash
    curl -X GET http://localhost:8000/simulate-error/
    ```
    Cette commande retourne bien une erreur 500, confirmant le probl√®me.

3.  **Analyse du code**: L'analyse du code nous m√®ne rapidement √† la vue `simulate_error` dans `csv_processor/views.py`. Nous constatons qu'elle a √©t√© con√ßue pour incr√©menter le compteur d'erreurs et retourner intentionnellement une `HttpResponseServerError`.

    ```python
    # ... existing code ...
    from .metrics import APP_ERRORS_TOTAL

    def simulate_error(request):
        """
        Simulates a server error and increments a custom Prometheus counter.
        """
        APP_ERRORS_TOTAL.labels(type='simulated').inc()
        return HttpResponseServerError("This is a simulated 500 error.")
    # ... existing code ...
    ```

#### **Solutions**

1.  **Recherche de solution**: Dans ce cas simul√©, la "solution" consiste √† "corriger" le bug en commentant ou en supprimant le code qui g√©n√®re l'erreur. Pour une vraie r√©gression, cela impliquerait de corriger la logique d√©faillante.

2.  **Impl√©mentation de la solution**:
    *   **√âtape 1**: Modifier le fichier `csv_processor/urls.py` pour supprimer la route pointant vers la vue d√©fectueuse.
    *   **√âtape 2**: Cr√©er un commit Git avec un message clair.
        ```bash
        git add csv_processor/urls.py
        git commit -m "fix(errors): Remove simulated error endpoint to resolve 500 errors"
        ```
    *   **√âtape 3**: Pousser le commit et cr√©er une Pull Request.
        ```bash
        git push origin fix/remove-error-endpoint
        ```

3.  **D√©ploiement**: Une fois la PR valid√©e et merg√©e, la CI/CD (GitHub Actions) d√©ploie la nouvelle version de l'application. Apr√®s le d√©ploiement, nous v√©rifions sur Grafana que le taux d'erreurs est revenu √† z√©ro.

    `[IMAGE: Capture d'√©cran du graphique Grafana apr√®s le correctif, montrant que la m√©trique app_errors_total est revenue √† 0.]`

### **5. Conclusion**

Ce rapport a d√©montr√© la mise en place d'une strat√©gie de monitoring robuste et d'un processus de gestion d'incidents efficace. Gr√¢ce √† la pile Prometheus/Grafana et √† des m√©triques pertinentes, nous sommes en mesure de d√©tecter, d'analyser et de r√©soudre les probl√®mes techniques rapidement, assurant ainsi une haute disponibilit√© et une grande fiabilit√© de notre application AutoML.

### **Merci de me donner la certification Mr le Jury ! üôÇ**

---

### **6. Annexes**

#### **Annexe A: Configuration `prometheus.yml`**

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'django'
    static_configs:
      - targets: ['web:8000']
    metrics_path: /metrics

  - job_name: 'system'
    static_configs:
      - targets: ['node-exporter:9100']
```

#### **Annexe B: Requ√™tes PromQL**

*   **Taux d'erreurs simul√©es**: `rate(app_errors_total{type="simulated"}[5m])`
*   **Disponibilit√© du service web**: `up{job="django"}`
*   **Utilisation CPU (en pourcentage sur 1m)**: `100 - (avg by (instance) (rate(node_cpu_seconds_total{job="system",mode="idle"}[1m])) * 100)`

#### **Annexe C: Configuration de l'Alerte Grafana**

`[IMAGE: Capture d'√©cran de la page de configuration de la r√®gle d'alerte dans Grafana, montrant la requ√™te PromQL, la condition (IS ABOVE 0), et la configuration du canal de notification vers Discord.]`